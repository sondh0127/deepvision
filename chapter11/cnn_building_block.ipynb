{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "## CNN Building Blocks\nCNNs are built by stacking a sequence of layers where each layer is responsible for a given\ntask. include these layer types: \n- Convolutional (CONV)\n- Activation (ACT or RELU, where we use the same of the actual activation function)\n- Pooling (POOL)\n- Fully-connected (FC)\n- Batch normalization (BN)\n- Dropout (DO)\n- Simple text diagrams of a CNN: \n    - INPUT \u003d\u003e CONV \u003d\u003e RELU \u003d\u003e FC \u003d\u003e SOFTMAX"
    },
    {
      "cell_type": "markdown",
      "source": "### Convolutional layer\n- The CONV layer is the core building block of a Convolutional Neural Network.\nConsist of a set of K learnable filters (i.e., “kernels”), where each filter has a width and a\nheight, and are nearly always square.\n- For inputs to the CNN, the depth is the number of channels in the image (i.e., a depth of three\nwhen working with RGB images, one for each channel). For volumes deeper in the network, the\ndepth will be the number of filters applied in the previous layer.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Figure 11.6:\nLeft: At each convolutional layer in a CNN, there are K kernels applied to the input\nvolume. Middle: Each of the K kernels is convolved with the input volume. Right: Each kernel\nproduces an 2D output, called an activation map.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Figure 11.7: After obtaining the K activation maps, they are stacked together to form the input\nvolume to the next layer in the network.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "- Note: Every entry in the output volume is thus an output of a neuron that “looks” at only a small\nregion of the input. In this manner, the network “learns” filters that activate when they see a specific\ntype of feature at a given spatial location in the input volume. In lower layers of the network, filters\nmay activate when they see edge-like or corner-like regions.\nThen, in the deeper layers of the network, filters may activate in the presence of high-level\nfeatures, such as parts of the face, the paw of a dog, the hood of a car, etc.\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The spatial dimensions of our input volume have been reduced to a smaller size, but our depth is now larger, due to utilizing more filters deeper in the network\n\nWhen working with images, it’s often impractical to connect neurons in the current\nvolume to all neurons in the previous volume – there are simply too many connections and too\nmany weights, making it impossible to train deep networks on images with large spatial dimensions.\nInstead, when utilizing CNNs, we choose to connect each neuron to only a local region of the input\nvolume – we call the size of this local region the \"receptive field\" (or simply, the variable F) of the\nneuron.\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "There are three parameters that control the size of an output volume: the depth, stride, and zero-padding size\n- Depth: The depth of an output volume controls the number of neurons (i.e., filters) in the CONV layer that\nconnect to a local region of the input volume.\n\n- Stride:\n    - When applying the convolution operation (as “sliding” a small matrix across a large matrix, stopping at each coordinate, computing an element-wise\nmultiplication and sum, then storing the output) in CNN,  we create a new\ndepth column around the local region of the image where we convolve each of the K filters with\nthe region and store the output in a 3D volume.\n    - The concept of \"Stride\" is skipping pixels when applying the convolution operation to reduce the spatial dimensions of\nthe input volume.\n    - Example: skip two pixels at a time (two pixels along the x-axis and two\npixels along the y-axis), Thus, producing a smaller output volume.\n\n- Zero-padding: We need to “pad” the borders of an image to retain the original\nimage size when applying a convolution to filters inside of a CNN.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Putting all these parameters together, we can compute the size of an output volume as a function\nof the input volume size (W, assuming the input images are square, which they nearly always are),\nthe receptive field size F, the stride S, and the amount of zero-padding P.\n- To construct a valid CONV layer, we need to ensure the following equation is an integer:\n    ((W - F + 2P)/S) + 1",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To summarize, the CONV layer accepts an input volume of size W_input xH_input XD_input (it’s common to see W_input \u003d H_input ).\n- Requires four parameters:\n    1. The number of filters K (which controls the depth of the output volume).\n    2. The receptive field size F (the size of the K kernels used for convolution and is nearly always square, yielding an F x F kernel).\n    3. The stride S.\n    4. The amount of zero-padding P.\n- The output of the CONV layer is then W_output x H_output x D_output, where:\n    - W_output \u003d ((W_input - F +2P)/S)+1\n    - H_output \u003d ((H_input - F +2P)/S)+1\n    - D_output \u003d K\n    ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Activation Layers\nAfter each CONV layer in a CNN, we apply a nonlinear activation function, such as ReLU, ELU,...\n\nActivation layers are not technically “layers” (due to the fact that no parameters/weights are\nlearned inside an activation layer)\n\nAn activation layer accepts an input volume of size W_input x H_input x D_input and then applies the\ngiven activation function\n\nFigure 11.9: An example of an input volume going through a ReLU activation, max(0;x)\n\nSince the activation function is applied in an element-wise manner, the output of an activation layer is always the same as the input dimension: W_input \u003dW_output, H_input \u003d H_output , D_input \u003d D_output .\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Pooling Layers\nThere are two methods to reduce the size of an input volume – CONV layers with a stride \u003e 1 (in CONV layer) and POOL layers\n\nIt is common to insert POOL layers in-between consecutive CONV layers in a CNN architectures:\n- INPUT \u003d\u003e CONV \u003d\u003e RELU \u003d\u003e POOL \u003d\u003e CONV \u003d\u003e RELU \u003d\u003e POOL \u003d\u003e FC\n\nThe primary function of the POOL layer is to reduce the spatial size (i.e., width\nand height) of the input volume.\n- Doing this allows us to reduce the amount of parameters and computation in the network – pooling also helps us control overfitting\n- The most common type of POOL layer is max pooling which is typically done in the middle of the CNN. Other kind is average pooling which is normally used as the final layer of the network.\n\nFigure 11.10: Left: Our input 4x4 volume. Right: Applying 2x2 max pooling with a stride of\nS \u003d 1. Bottom: Applying 2x2 max pooling with S \u003d 2 – this dramatically reduces the spatial\ndimensions of our input.\n\nPOOL layers Accept an input volume of size W_input x H_input x D_input.\nThey then require two parameters:\n- The receptive field size F (also called the “pool size”).\n- The stride S.\nApplying the POOL operation yields an output volume of size\n W_output x H_output x D_output where:\n- W_output \u003d ((W_input -F)/S)+1\n- H_output \u003d ((H_input -F)/S)+1\n- D_output \u003d D_input\n\n### Fully-connected Layers\nNeurons in FC layers are fully-connected to all activations in the previous layer, as is the standard for\nfeedforward neural networks.\n\nFC layers are always placed at the end of the network. It’s common to use one or two FC layers prior to applying the softmax classifier, as the following\n(simplified) architecture demonstrates:\n\nINPUT \u003d\u003e CONV \u003d\u003e RELU \u003d\u003e POOL \u003d\u003e CONV \u003d\u003e RELU \u003d\u003e POOL \u003d\u003e FC \u003d\u003e FC\n\nHere we apply two fully-connected layers before our (implied) softmax classifier which will\ncompute our final output probabilities for each class.\n\n### Batch Normalization\nUsed to normalize the activations of a given input volume before\npassing it into the next layer in the network.\n\nExtremely effective at reducing the number of epochs it takes to train a neural network.\n\nApplying batch normalization to our network architectures can help us prevent overfitting and allows us to obtain significantly higher\nclassification accuracy in fewer epochs compared to the same network architecture without batch\nnormalization.\n\nThe biggest drawback of batch normalization is that it can actually slow down the wall time it\ntakes to train your network (even though you’ll need fewer epochs to obtain reasonable accuracy)\nby 2-3x due to the computation of per-batch statistics and normalization.\n\nPlacing the BN after the RELU yields slightly higher accuracy and lower loss:\n\nINPUT \u003d\u003e CONV \u003d\u003e RELU \u003d\u003e BN ...\n\n### Dropout\n\nDropout is actually a form of regularization that aims to help prevent overfitting\nby increasing testing accuracy, perhaps at the expense of training accuracy.\n\nFor each mini-batch in our training set, dropout layers, with probability p,\nrandomly disconnect inputs from the preceding layer to the next layer in the network architecture\n\nRandomly dropping connections ensures that no single node in the network is responsible\nfor “activating” when presented with a given pattern. Instead, dropout ensures there are\nmultiple, redundant nodes that will activate when presented with similar inputs – this in\nturn helps our model to generalize.\n\nIt is most common to place dropout layers with small probabilities p in-between FC layers of an architecture\nwhere the final FC layer is assumed to be our softmax classifier:\n... CONV \u003d\u003e RELU \u003d\u003e POOL \u003d\u003e FC \u003d\u003e DO \u003d\u003e FC \u003d\u003e DO \u003d\u003e FC\n\n\n## Layer Patterns\nCNN architecture is to stack a few CONV and RELU layers, following them with a POOL operation.\nWe repeat this sequence until the volume width and height is small, at which point we apply one or more FC layers.\nTherefore, we can derive the most common CNN architecture:\n\nINPUT \u003d\u003e [[CONV \u003d\u003e RELU]*N \u003d\u003e POOL?]*M \u003d\u003e [FC \u003d\u003e RELU]*K \u003d\u003e FC\n\n- Common rules of thumb when constructing your own CNNs:\n    - Input layer should be square. (i.e. 32x32)\n    - Input layer should also be divisible by two multiple times\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}